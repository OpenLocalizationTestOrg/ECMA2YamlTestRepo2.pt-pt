### YamlMime:ManagedReference
items:
- uid: System.Speech.Recognition.RecognizedAudio
  id: RecognizedAudio
  children:
  - System.Speech.Recognition.RecognizedAudio.AudioPosition
  - System.Speech.Recognition.RecognizedAudio.Duration
  - System.Speech.Recognition.RecognizedAudio.Format
  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  - System.Speech.Recognition.RecognizedAudio.StartTime
  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  langs:
  - csharp
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
  type: Class
  summary: "Áudio representa de entrada que está associado um <xref href=&quot;System.Speech.Recognition.RecognitionResult&quot;> </xref>."
  remarks: "Um utilitário de reconhecimento de voz gera as informações sobre a entrada de áudio como parte da operação de reconhecimento. Para aceder o áudio reconhecido, utilize a <xref:System.Speech.Recognition.RecognitionResult.Audio%2A>propriedade ou o <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>método de <xref:System.Speech.Recognition.RecognitionResult>.</xref:System.Speech.Recognition.RecognitionResult> </xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A>       Um resultado de reconhecimento pode ser produzido pela seguintes eventos e métodos para a <xref:System.Speech.Recognition.SpeechRecognizer>e <xref:System.Speech.Recognition.SpeechRecognitionEngine>classes:-eventos:- <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>-métodos:- <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName>e <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>> [!IMPORTANT] > reconhecimento um resultado produzido pelo reconhecimento de voz emulados não contém áudio reconhecido.</xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer> Para tal um resultado de reconhecimento, respetivo <xref:System.Speech.Recognition.RecognitionResult.Audio%2A>propriedade devolve `null` e a respetiva <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>método emite uma exceção.</xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Para mais informações sobre o reconhecimento de voz emulados, consulte o `EmulateRecognize` e `EmulateRecognizeAsync` métodos para a <xref:System.Speech.Recognition.SpeechRecognizer>e <xref:System.Speech.Recognition.SpeechRecognitionEngine>classes.</xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public class RecognizedAudio
  inheritance:
  - System.Object
  implements: []
  inheritedMembers: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  id: AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Obtém a localização na sequência de áudio entrada para o início do áudio reconhecido."
  remarks: "Esta propriedade referencia a posição no início do frase reconhecido em sequência de áudio gerado de entrada do dispositivo. Por outro lado, a `RecognizerAudioPosition` propriedade o <xref:System.Speech.Recognition.SpeechRecognitionEngine>e <xref:System.Speech.Recognition.SpeechRecognizer>classes referenciam posição o reconhecedor a entrada de áudio.</xref:System.Speech.Recognition.SpeechRecognizer> </xref:System.Speech.Recognition.SpeechRecognitionEngine> Estes posições podem ser diferentes. Para obter mais informações, consulte [através de eventos de reconhecimento de voz](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482).       O <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>propriedade obtém a hora do sistema no início da operação de reconhecimento.</xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan AudioPosition { get; }
    return:
      type: System.TimeSpan
      description: "A localização na sequência de áudio entrada para o início de áudio reconhecido."
  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  id: Duration
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Obtém a duração da sequência de áudio entrada de áudio reconhecido."
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan Duration { get; }
    return:
      type: System.TimeSpan
      description: "A duração dentro do fluxo de áudio entrada de áudio reconhecido."
  overload: System.Speech.Recognition.RecognizedAudio.Duration*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Format
  id: Format
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Obtém o formato de áudio processado pelo motor de reconhecimento."
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }
    return:
      type: System.Speech.AudioFormat.SpeechAudioFormatInfo
      description: "O formato de áudio processado pelo utilitário de reconhecimento de voz."
  overload: System.Speech.Recognition.RecognizedAudio.Format*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  id: GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Seleciona e devolve uma secção de atual reconhecido áudio como dados binários."
  remarks: ''
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the wildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);
    parameters:
    - id: audioPosition
      type: System.TimeSpan
      description: "O ponto de partida dos dados de áudio a ser devolvido."
    - id: duration
      type: System.TimeSpan
      description: "O comprimento de segmento deve ser devolvida."
    return:
      type: System.Speech.Recognition.RecognizedAudio
      description: "Devolve uma subsecção de áudio reconhecido, conforme definido pelo <code> audioPosition </code> e <code> duration </code>."
  overload: System.Speech.Recognition.RecognizedAudio.GetRange*
  exceptions:
  - type: System.ArgumentOutOfRangeException
    commentId: T:System.ArgumentOutOfRangeException
    description: "<code>audioPosition</code>e <code>duration</code> definir um segmento de áudio fora do intervalo do segmento atual."
  - type: System.InvalidOperationException
    commentId: T:System.InvalidOperationException
    description: "Atual reconhecido áudio não contém dados."
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  id: StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Obtém a hora do sistema no início da operação de reconhecimento."
  remarks: "A propriedade de StartTime obtém a hora do sistema no início da operação de reconhecimento, que pode ser útil para cálculos de latência e o desempenho.       O <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>propriedade obtém a localização na sequência de áudio gerado de entrada do dispositivo.</xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public DateTime StartTime { get; }
    return:
      type: System.DateTime
      description: "A hora do sistema no início da operação de reconhecimento."
  overload: System.Speech.Recognition.RecognizedAudio.StartTime*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  id: WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Escreve áudio completo para uma transmissão em fluxo como dados não processados."
  remarks: "Dados de áudio são escritos `outputStream` no formato binário. Não existem informações de cabeçalho estão incluídas.       O método WriteToAudioStream utiliza o formato de Wave, mas não inclui o cabeçalho da Wave. Para incluir o cabeçalho da Wave, utilize o <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>método.</xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>"
  syntax:
    content: public void WriteToAudioStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "O fluxo que irá receber os dados de áudio."
  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  id: WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Escreve áudio um fluxo no formato Wave."
  remarks: "Dados de áudio são escritos `outputStream` no formato Wave, que inclui um cabeçalho de formato (RIFF) de ficheiro de intercâmbio de recursos.       O <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>método utiliza o mesmo formato binário, mas não inclui o cabeçalho da Wave.</xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>"
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the   \nwildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public void WriteToWaveStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "O fluxo que irá receber os dados de áudio."
  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  exceptions: []
  platform:
  - net462
references:
- uid: System.Object
  isExternal: false
  name: System.Object
- uid: System.ArgumentOutOfRangeException
  isExternal: true
  name: System.ArgumentOutOfRangeException
- uid: System.InvalidOperationException
  isExternal: true
  name: System.InvalidOperationException
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
- uid: System.TimeSpan
  parent: System
  isExternal: true
  name: TimeSpan
  nameWithType: TimeSpan
  fullName: System.TimeSpan
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo
  parent: System.Speech.AudioFormat
  isExternal: false
  name: SpeechAudioFormatInfo
  nameWithType: SpeechAudioFormatInfo
  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
- uid: System.Speech.Recognition.RecognizedAudio
  parent: System.Speech.Recognition
  isExternal: false
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
- uid: System.DateTime
  parent: System
  isExternal: true
  name: DateTime
  nameWithType: DateTime
  fullName: System.DateTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
- uid: System.IO.Stream
  parent: System.IO
  isExternal: true
  name: Stream
  nameWithType: Stream
  fullName: System.IO.Stream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
- uid: System.Speech.Recognition.RecognizedAudio.Duration*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
- uid: System.Speech.Recognition.RecognizedAudio.GetRange*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange
  nameWithType: RecognizedAudio.GetRange
- uid: System.Speech.Recognition.RecognizedAudio.StartTime*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream
  nameWithType: RecognizedAudio.WriteToAudioStream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream
  nameWithType: RecognizedAudio.WriteToWaveStream
